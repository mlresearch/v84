---
title: Online Learning with Non-Convex Losses and Non-Stationary Regret
abstract: In this paper, we consider online learning with non-convex loss functions.
  Similar to Besbes et al. [2015] we apply non-stationary regret as the performance
  metric. In particular, we study the regret bounds under different assumptions on
  the information available regarding the loss functions. When the gradient of the
  loss function at the decision point is available, we propose an online normalized
  gradient descent algorithm (ONGD) to solve the online learning problem. In another
  situation, when only the value of the loss function is available, we propose a bandit
  online normalized gradient descent algorithm (BONGD). Under a condition to be called
  weak pseudo-convexity (WPC), we show that both algorithms achieve a cumulative regret
  bound of O($\sqrt{T+V_T T}$), where $V_T$ is the total temporal variations of the
  loss functions, thus establishing a sublinear regret bound for online learning with
  non-convex loss functions and non-stationary regret measure.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: gao18a
month: 0
tex_title: Online Learning with Non-Convex Losses and Non-Stationary Regret
firstpage: 235
lastpage: 243
page: 235-243
order: 235
cycles: false
author:
- given: Xiand
  family: Gao
- given: Xiaobo
  family: Li
- given: Shuzhong
  family: Zhang
date: 2018-03-31
address: 
publisher: PMLR
container-title: Proceedings of the Twenty-First International Conference on Artficial
  Intelligence and Statistics
volume: '84'
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 3
  - 31
pdf: http://proceedings.mlr.press/v84/gao18a/gao18a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v84/gao18a/gao18a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
