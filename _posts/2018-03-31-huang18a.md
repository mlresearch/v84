---
title: A Unified Dynamic Approach to Sparse Model Selection
abstract: Sparse model selection is ubiquitous from linear regression to graphical
  models where regularization paths, as a family of estimators upon the regularization
  parameter varying, are computed when the regularization parameter is unknown or
  decided data-adaptively. Traditional computational methods rely on solving a set
  of optimization problems where the regularization parameters are fixed on a grid
  that might be inefficient. In this paper, we introduce a simple iterative regularization
  path, which follows the dynamics of a sparse Mirror Descent algorithm or a generalization
  of Linearized Bregman Iterations with nonlinear loss. Its performance is competitive
  to glmnet with a further bias reduction. A path consistency theory is presented
  that under the Restricted Strong Convexity and the Irrepresentable Condition, the
  path will first evolve in a subspace with no false positives and reach an estimator
  that is sign-consistent or of minimax optimal $\ell_2$ error rate. Early stopping
  regularization is required to prevent overfitting. Application examples are given
  in sparse logistic regression and Ising models for NIPS coauthorship.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: huang18a
month: 0
tex_title: A Unified Dynamic Approach to Sparse Model Selection
firstpage: 2047
lastpage: 2055
page: 2047-2055
order: 2047
cycles: false
author:
- given: Chendi
  family: Huang
- given: Yuan
  family: Yao
date: 2018-03-31
address: 
publisher: PMLR
container-title: Proceedings of the Twenty-First International Conference on Artficial
  Intelligence and Statistics
volume: '84'
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 3
  - 31
pdf: http://proceedings.mlr.press/v84/huang18a/huang18a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
