---
title: Scalable Hash-Based Estimation of Divergence Measures
abstract: We propose a scalable divergence estimation method based on hashing. Consider
  two continuous random variables $X$ and $Y$ whose densities have bounded support.
  We consider a particular locality sensitive random hashing, and consider the ratio
  of samples in each hash bin having non-zero numbers of Y samples. We prove that
  the weighted average of these ratios over all of the hash bins converges to f-divergences
  between the two samples sets. We derive the MSE rates for two families of smooth
  functions; the HÃ¶lder smoothness class and differentiable functions. In particular,
  it is proved that if the density functions have bounded derivatives up to the order
  $d$, where $d$ is the dimension of samples, the optimal parametric MSE rate of $O(1/N)$
  can be achieved.  The computational complexity is shown to be $O(N)$, which is optimal.
  To the best of our knowledge, this is the first empirical divergence estimator that
  has optimal computational complexity and can achieve the optimal parametric MSE
  estimation rate of $O(1/N)$.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: noshad18a
month: 0
tex_title: Scalable Hash-Based Estimation of Divergence Measures
firstpage: 1877
lastpage: 1885
page: 1877-1885
order: 1877
cycles: false
author:
- given: Morteza
  family: Noshad
- given: Alfred
  family: Hero
date: 2018-03-31
address: 
publisher: PMLR
container-title: Proceedings of the Twenty-First International Conference on Artficial
  Intelligence and Statistics
volume: '84'
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 3
  - 31
pdf: http://proceedings.mlr.press/v84/noshad18a/noshad18a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
