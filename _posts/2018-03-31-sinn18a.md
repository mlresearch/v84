---
title: Non-parametric estimation of Jensen-Shannon Divergence in Generative Adversarial
  Network training
abstract: 'Generative Adversarial Networks (GANs) have become a widely popular framework
  for generative modelling of high-dimensional datasets. However their training is
  well-known to be difficult. This work presents a rigorous statistical analysis of
  GANs providing straight-forward explanations for common training pathologies such
  as vanishing gradients. Furthermore, it proposes a new training objective, Kernel
  GANs and demonstrates its practical effectiveness on large-scale real-world data
  sets. A key element in the analysis is the distinction between training with respect
  to the (unknown) data distribution, and its empirical counterpart. To overcome issues
  in GAN training, we pursue the idea of smoothing the Jensen-Shannon Divergence (JSD)
  by incorporating noise in the input distributions of the discriminator. As we show,
  this effectively leads to an empirical version of the JSD in which the true and
  the generator densities are replaced by kernel density estimates, which leads to
  Kernel GANs. '
layout: inproceedings
series: Proceedings of Machine Learning Research
id: sinn18a
month: 0
tex_title: Non-parametric estimation of Jensen-Shannon Divergence in Generative Adversarial
  Network training
firstpage: 642
lastpage: 651
page: 642-651
order: 642
cycles: false
author:
- given: Mathieu
  family: Sinn
- given: Ambrish
  family: Rawat
date: 2018-03-31
address: 
publisher: PMLR
container-title: Proceedings of the Twenty-First International Conference on Artficial
  Intelligence and Statistics
volume: '84'
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 3
  - 31
pdf: http://proceedings.mlr.press/v84/sinn18a/sinn18a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v84/sinn18a/sinn18a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
